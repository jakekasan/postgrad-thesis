\section{Review of Work}

\subsection{Intention}
This workâ€™s aim was two-fold: perform panel data unit root tests on real-world data and determine which of the panel data tests perform better under which conditions. In regards to the first aim, this project succeeded. The given data was tested both with single time series unit root tests and panel data unit root tests, and it was found that while the single time series tests largely indicated that all the series were non-stationary, as the individual dimension of the panels grew, the panel unit root tests increasingly found that the panels were stationary.

In terms of the simulations, panels of varying dimensions were created, tested individually and then subjected to panel unit root tests in a Monte Carlo simulation, where each iteration of the simulation involved creating a new time series using the same process. All the panel data tests were found to react to the changes in dimension by way of either reduced or increased power. The panels where both T and N were large produced less type I errors and type II errors. When T and N were not large, the panels would always fail to reject the null, but as the dimensions were slowly increased, the rate of change in the p-value produced by the tests varied from test to test. It was found that the Levin-Lin-Chu had the most desirable performance, while the Maddala-Wu tended towards type I errors and the Im-Perasan-Shin did not behave in an explicable way as the dimensions were increased. 

But how to interpret these results? The first step is to generate an understanding of the relationship between the test results and the characteristics of the input data. It appears that Fisher-type tests, which are simply an aggregation of p-values from individual tests, are far too likely to pronounce a panel as unstationary, even when the underlying process of the individual time series is a deterministic process. This means that if the Maddala-Wu was relied upon to test real-world data, the inferences formed on all subsequent tests done on the panel would be flawed, because nearly all meaningful statistical inferences require the data to be at least trend stationary. The Im-Pesaran-Shin, which is inspired by the Fisher-type tests except that critical values are averaged out instead of p-values, performed even worse.

\subsection{Flaws With Methodology}

A way to improve on the methodology in this thesis is to increase the number of panel unit root tests which were run. Due to software and time limitations, this work relied upon external libraries to supply panel data unit root tests. Given that these tests are a relatively new field, especially those which account for cross-sectional dependence, it is understandable that they would not be readily available to use in pre-programmed form. What is less understandable, however, is the fact that a test was implemented incorrectly, attempting to use a normal distribution to interpret a Dickey-Fuller statistic. Having said that, it would not take a great deal of time to create a robust package which facilitated these tests (robust here meaning that the package would be able to deal with all the nuances expected of software packages, such as error reporting, dealing with different input data types, etc), just more time than was afforded to this thesis. As a result, this work merely opened the door to question posed by it.


Another issue with the work was that some time was spent correcting issues with the pre-existing packages. Notably, the implementation of the Maddala-Wu test in the "plm" package was incorrect. As was discussed in Chapter 2, the Maddala-Wu is a Fisher-type test, which in the way it was implemented in R tested each individual with the Augmented Dickey-Fuller test, generated p-values and then applied the formula 2.10 in order to generate the t-statistic. The issue with this implementation is two-fold: the calculation of the p-values and the input limitations. The p-values were calculated by looking up the critical value on a normal distribution, but $\cite{df}$ clearly state that there is a specific Dickey-Fuller distribution which is to be used. As a result, the p-values generated by the test are invalid. In addition, the implementation of the test required that the input data be in a balanced format, which is not required in the actual test as detailed by $\cite{mw}$. The way that this was resolved, however, was that a custom version of the test was developed, which compared the critical values against Dickey-Fuller distributions that were manually generated through a Monte Carlo simulation. This corrected for the p-values. The second problem is more to do with the R programming language conventions, where the Data-Frame, the conventional way of storing panels, cannot by definition be unbalanced. This was not actually an issue in the implementation of the methodology, as this created balanced panels to ensure comparability with other panel data tests which require balanced panels in the literature. Furthermore, an easy way to overcome the Data-Frame limitation is to manually balance the panels by introducing "NA" variables for the shorter individuals until the panel is balanced.




One way in which the findings here would be useful for industry is the affirmation that the cross section affords effectively a larger sample size $\cite{smith}$. This is useful because for certain metrics, which are reported quarterly, 12 observations is three years, and for certain products or industries this may be the limit of time that a metric is either available or advisable. Therefore the time dimension is capped at 12 observations or so, which is not ideal considering that standard unit root tests such as the ADF or PP only really begin to have power at an excess of 100 observations. However with panel data, if more individuals are sourced, it may be possible to achieve a meaningful test result even if the time dimension is not generous. Indeed this was the idea with panel data stationarity tests in the first place. Being originally designed for examining Purchasing Power Parity $\cite{oh1996purchasing}$, the intuition behind panel data was that countries could be sampled multiple times by way of using multiple metrics for the economic development of a country (GDP growth, unemployment, inflation, etc), because these metrics should in theory be representing the same process, ergo their inclusion would increase the sample size for that process, even when the time dimension may be limited. 



\section{Suggestions for Future Research}

The main way in which the research done here could be extended is an exploration into the second generation panel unit root tests, such as the one proposed by $\cite{panic04}$. Although the tests utilized in this work showed good power in the circumstances in which they were tested ($T \to 0$ and $N \to 0$), particularly the Levin-Lin-Chu, if it is assumed that the panels supplied are all generated by the same basic process but exhibit sampling error, the optimal way to test and model these processes could be to split the process into a communal factor-driven process and an individual-specific processes. The common component could be a matrix of macro indicators and their lags deemed to be statistically significant, which would not only eliminate cross-sectional dependencies $\cite{hurlin}$ but also allow for more accurate forecasting of the data. The challenge for this approach is sourcing the common factors and individual $\epsilon_t$ terms, which could be overcome if the correct indicators and their respective lags were used. Another test which would be very relevant for the scenario where T is limited is the Harris and Tzavalis test \citep{harris1999inference}. This test was developed for instances where T is fixed while $N \to \infty$, which is ideal especially for macroeconomic studies, where using decades of data may not be desirable or possible. This test was meant to be implemented and tested for this investigation, but the amount of time spent correcting the existing issues meant that it was not possible in the given time frame.